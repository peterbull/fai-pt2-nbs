{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(5, 3)  # first linear transformation\n",
    "        self.fc2 = torch.nn.Linear(3, 1)  # second linear transformation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  # first activation layer\n",
    "        x = self.fc2(x)\n",
    "        output = torch.sigmoid(x)  # second activation layer (sigmoid to output probabilities)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I progress through Part 2 of the fastai course, I've discovered the value of occasionally retracing my steps to assess my grasp on the fundamental concepts. In my journey through Part 1, I recall Chapter 4 of the book being particularly daunting, as it covered a broad range of essential topics early on. However, this comprehensive approach was necessary to establish a strong foundation for everything I would subsequently learn.\n",
    "\n",
    "Whenever I revisit those earlier chapters and analyze the code line by line, I'm often pleasantly surprised by the level of intuition I've developed in certain areas. Simultaneously, I'm also confronted with aspects that still elude my immediate understanding. Engaging in quick drills to reinforce these concepts has proven to be an effective method for staying sharp and achieving complete comprehension, even if it may feel somewhat repetitive. It's like eating vegetables as a child â€“ you may not want to, but it's ultimately beneficial and necessary for growth. I believe that through a similar positive feedback loop, I will acquire a taste for this process and derive great rewards from it.\n",
    "\n",
    "So, grab a fork and let's dig in"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Bank Loan Default from Synthetic Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise I'm going to create synthetic data to predict whether a bank loan will default. I've chosen the following to create as features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loan Amount \n",
    "- Term \n",
    "- Interest Rate \n",
    "- Borrower's Income \n",
    "- Borrower's Credit Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're first going to create the data for 1000 samples randomly with `torch.normal`, to which we'll pass arguments for a *mean*, *standard deviation*, and *size*.\n",
    "\n",
    "The size could be either a tuple or a list, but for this exercise we'll just use a tuple with a single value.\n",
    "\n",
    "Also worth noting that `term` will be using `randint` rather than `normal`, because loan terms are generally in whole months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "loan_amount = torch.normal(5000., 1500, size=(n_samples,))  # average loan amount is $5000\n",
    "term = torch.randint(12, 60, size=(n_samples,))  # loan term varies between 1 and 5 years\n",
    "interest_rate = torch.normal(0.05, 0.01, size=(n_samples,))  # average interest rate is 5%\n",
    "income = torch.normal(50000, 10000, size=(n_samples,))  # average income is $50,000\n",
    "credit_score = torch.normal(600, 50, size=(n_samples,))  # average credit score is 600"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll stack the tensors and create a target variable where 10% of loans default, and convert them to *float32* data types in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([loan_amount, term, interest_rate, income, credit_score]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.distributions.categorical.Categorical(torch.tensor([0.9, 0.1])).sample((n_samples,)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
